import fitz  # PyMuPDF
import json
import os

def normalize_farsi_text(text):
    replacements = {
        '\u0640': '',         # Ú©Ø´ÛŒØ¯Ù‡
        'ÙŠ': 'ÛŒ',
        'Ùƒ': 'Ú©',
        'â€Œ': ' ',             # Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ø¨Ù‡ ÙØ§ØµÙ„Ù‡
        '\u200c': ' ',         # Zero-width non-joiner
        '//' : '',
        '\n': '',            # Ø³Ø·Ø± Ø¬Ø¯ÛŒØ¯ â†’ ÙØ§ØµÙ„Ù‡
    }
    for src, tgt in replacements.items():
        text = text.replace(src, tgt)
    return text

def extract_paragraphs_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    paragraphs = []
    current_paragraph = ""

    for page in doc:
        lines = page.get_text("text").splitlines()
        for line in lines:
            clean_line = normalize_farsi_text(line.strip())
            if not clean_line:
                # Ø®Ø· Ø®Ø§Ù„ÛŒ: Ù¾Ø§ÛŒØ§Ù† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                if current_paragraph:
                    paragraphs.append(current_paragraph.strip())
                    current_paragraph = ""
            else:
                # Ø§Ø¯Ø§Ù…Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                if current_paragraph:
                    current_paragraph += " " + clean_line
                else:
                    current_paragraph = clean_line

    if current_paragraph:
        paragraphs.append(current_paragraph.strip())  # Ø¢Ø®Ø±ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù

    # Ø­Ø°Ù Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ ÛŒØ§ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†Ø§
    paragraphs = [p for p in paragraphs if len(p) > 50]

    print(f"ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² {pdf_path}: {len(paragraphs)} Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù ÙˆØ§Ù‚Ø¹ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
    return paragraphs

def create_summarization_dataset(paragraphs, output_path):
    dataset = []
    for para in paragraphs:
        item = {
            "input": f"Ø®Ù„Ø§ØµÙ‡ Ú©Ù†: {para}",
            "target": ""  # Ø¨Ø¹Ø¯Ø§Ù‹ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø³ØªÛŒ ÛŒØ§ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒ
        }
        dataset.append(item)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in dataset:
            json.dump(item, f, ensure_ascii=False)
            f.write('\n')

    print(f"âœ… Ø®Ø±ÙˆØ¬ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_path} (ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(dataset)})")

def main():
    input_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "pdf")
    base_dir = os.path.dirname(os.path.abspath(__file__))  # Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡â€ŒÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª
    output_dir = os.path.join(base_dir, "..", "source")
    output_dir = os.path.abspath(output_dir)  # Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†ØŒ Ù…Ø·Ù„Ù‚â€ŒØ³Ø§Ø²ÛŒ Ù…Ø³ÛŒØ±

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for filename in os.listdir(input_dir):
        if filename.lower().endswith(".pdf"):
            pdf_path = os.path.join(input_dir, filename)
            base_name = os.path.splitext(filename)[0]
            output_path = os.path.join(output_dir, f"{base_name}.jsonl")

            print(f"ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {filename}")
            paragraphs = extract_paragraphs_from_pdf(pdf_path)
            if paragraphs:
                create_summarization_dataset(paragraphs, output_path)
            else:
                print(f"âš ï¸ Ù‡ÛŒÚ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± ÙØ§ÛŒÙ„ {filename} ÛŒØ§ÙØª Ù†Ø´Ø¯.")

if __name__ == "__main__":
    main()
