import fitz  # PyMuPDF
import json
import os

def normalize_farsi_text(text):
    replacements = {
        '\u0640': '',         # Ú©Ø´ÛŒØ¯Ù‡
        'ÙŠ': 'ÛŒ',
        'Ùƒ': 'Ú©',
        'â€Œ': ' ',             # Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ø¨Ù‡ ÙØ§ØµÙ„Ù‡
        '\u200c': ' ',         # Zero-width non-joiner
        '//' : '',
        '\n': '',            # Ø³Ø·Ø± Ø¬Ø¯ÛŒØ¯ â†’ ÙØ§ØµÙ„Ù‡
    }
    for src, tgt in replacements.items():
        text = text.replace(src, tgt)
    return text

def extract_paragraphs_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    paragraphs = []
    current_paragraph = ""

    for page in doc:
        lines = page.get_text("text").splitlines()
        for line in lines:
            clean_line = normalize_farsi_text(line.strip())

            # Ø­Ø°Ù Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡ Ø³Ø§Ø¯Ù‡
            if clean_line.isdigit():
                continue

            if not clean_line:
                # Ø®Ø· Ø®Ø§Ù„ÛŒ: Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø§ÛŒØ§Ù† Ù…Ù†Ø·Ù‚ÛŒ
                if current_paragraph:
                    if current_paragraph.strip()[-1:] in ".!ØŸ":
                        paragraphs.append(current_paragraph.strip())
                        current_paragraph = ""
                    else:
                        continue
            else:
                # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø®Ø· Ø¨Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù ÙØ¹Ù„ÛŒ
                if current_paragraph:
                    current_paragraph += " " + clean_line
                else:
                    current_paragraph = clean_line

                # âœ… Ø´Ø±Ø· Ø¬Ø¯ÛŒØ¯: Ø§Ú¯Ø± Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù ÙØ¹Ù„ÛŒ Ø¨ÛŒØ´ Ø§Ø² 300 Ú©Ù„Ù…Ù‡ Ø¯Ø§Ø±Ø¯ Ùˆ Ø¨Ø§ Ù†Ù‚Ø·Ù‡ ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ØŒ Ø¢Ù† Ø±Ø§ Ù‚Ø·Ø¹ Ú©Ù†ÛŒÙ…
                word_count = len(current_paragraph.split())
                if word_count >= 300 and current_paragraph.strip()[-1:] in ".!ØŸ":
                    paragraphs.append(current_paragraph.strip())
                    current_paragraph = ""

                    clean_line = normalize_farsi_text(line.strip())

                    # Ø­Ø°Ù Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡ ÛŒØ§ ØªÛŒØªØ±
                    if clean_line.isdigit() or len(clean_line) < 6 and all(c.isdigit() or c in 'ÙØµÙ„ ' for c in clean_line):
                        continue

                    if not clean_line:
                        # Ø®Ø· Ø®Ø§Ù„ÛŒ: Ù¾Ø§ÛŒØ§Ù† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                        if current_paragraph:
                            paragraphs.append(current_paragraph.strip())
                            current_paragraph = ""
                    else:
                        # Ø§Ø¯Ø§Ù…Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                        if current_paragraph:
                            current_paragraph += " " + clean_line
                        else:
                            current_paragraph = clean_line

        # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø§ÛŒØ§Ù† ØµÙØ­Ù‡:
        if current_paragraph and current_paragraph.strip()[-1:] not in ".!ØŸ":
            # Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ù‡Ù†ÙˆØ² ØªÙ…ÙˆÙ… Ù†Ø´Ø¯Ù‡ â†’ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù‡
            continue
        elif current_paragraph:
            # Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ú©Ø§Ù…Ù„ Ø´Ø¯Ù‡ â†’ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
            paragraphs.append(current_paragraph.strip())
            current_paragraph = ""

    if current_paragraph:
        paragraphs.append(current_paragraph.strip())  # Ø¢Ø®Ø±ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù

    # Ø­Ø°Ù Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ ÛŒØ§ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†Ø§
    paragraphs = [p for p in paragraphs if len(p) > 50]

    print(f"ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² {pdf_path}: {len(paragraphs)} Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù ÙˆØ§Ù‚Ø¹ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
    return paragraphs
    doc = fitz.open(pdf_path)
    paragraphs = []
    current_paragraph = ""

    for page in doc:
        lines = page.get_text("text").splitlines()
        for line in lines:
            if clean_line.isdigit() or len(clean_line) < 20 and all(c.isdigit() or c in 'ÙØµÙ„ ' for c in clean_line):
                continue  # Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø´Ù…Ø§Ø±Ù‡ ØµÙØ­Ù‡ ÛŒØ§ ØªÛŒØªØ± Ø¨ÛŒâ€ŒØ±Ø¨Ø·
            clean_line = normalize_farsi_text(line.strip())
            if not clean_line:
                # Ø®Ø· Ø®Ø§Ù„ÛŒ: Ù¾Ø§ÛŒØ§Ù† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                if current_paragraph:
                    paragraphs.append(current_paragraph.strip())
                    current_paragraph = ""
            else:
                # Ø§Ø¯Ø§Ù…Ù‡ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                if current_paragraph:
                    current_paragraph += " " + clean_line
                else:
                    current_paragraph = clean_line

    if current_paragraph:
        paragraphs.append(current_paragraph.strip())  # Ø¢Ø®Ø±ÛŒÙ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù

    # Ø­Ø°Ù Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ ÛŒØ§ Ø¨ÛŒâ€ŒÙ…Ø¹Ù†Ø§
    paragraphs = [p for p in paragraphs if len(p) > 50]

    print(f"ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² {pdf_path}: {len(paragraphs)} Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù ÙˆØ§Ù‚Ø¹ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯")
    return paragraphs

def create_summarization_dataset(paragraphs, output_path):
    dataset = []
    for para in paragraphs:
        item = {
            "input": f"Ø®Ù„Ø§ØµÙ‡ Ú©Ù†: {para}",
            "target": ""  # Ø¨Ø¹Ø¯Ø§Ù‹ Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø³ØªÛŒ ÛŒØ§ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒ
        }
        dataset.append(item)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in dataset:
            json.dump(item, f, ensure_ascii=False)
            f.write('\n')

    print(f"âœ… Ø®Ø±ÙˆØ¬ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_path} (ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {len(dataset)})")

def main():
    input_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "pdf")
    base_dir = os.path.dirname(os.path.abspath(__file__))  # Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡â€ŒÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª
    output_dir = os.path.join(base_dir, "..", "source")
    output_dir = os.path.abspath(output_dir)  # Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù†ØŒ Ù…Ø·Ù„Ù‚â€ŒØ³Ø§Ø²ÛŒ Ù…Ø³ÛŒØ±

    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    for filename in os.listdir(input_dir):
        if filename.lower().endswith(".pdf"):
            pdf_path = os.path.join(input_dir, filename)
            base_name = os.path.splitext(filename)[0]
            output_path = os.path.join(output_dir, f"{base_name}.jsonl")

            print(f"ğŸ”„ Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {filename}")
            paragraphs = extract_paragraphs_from_pdf(pdf_path)
            if paragraphs:
                create_summarization_dataset(paragraphs, output_path)
            else:
                print(f"âš ï¸ Ù‡ÛŒÚ† Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± ÙØ§ÛŒÙ„ {filename} ÛŒØ§ÙØª Ù†Ø´Ø¯.")

if __name__ == "__main__":
    main()
